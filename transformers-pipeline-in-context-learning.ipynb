{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f77650c0-d121-450c-aa4a-edf9ced42d2a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-11 10:32:49.121923: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-11-11 10:32:49.984493: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64\n",
      "2023-11-11 10:32:49.984654: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64\n",
      "2023-11-11 10:32:49.984661: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "035f457e-3aa6-4ef9-801f-9e6b8d80f28a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'4.35.0'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transformers.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8c183c15-25fe-4f77-8284-f0b5df90498b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Install transformers 4.35.0 -> earlier versions have problem with zephir (KeyError mistral)\n",
    "#!pip install transformers -U"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "080179b7-5275-4807-a555-387e3ff0444d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def in_context_prompt(question, context):\n",
    "    return f\"\"\"Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
    "\n",
    "                {context}\n",
    "\n",
    "                Question: {question}\"\"\"\n",
    "\n",
    "\n",
    "class LLMPipeline(Pipeline):\n",
    "    def _sanitize_parameters(self, **kwargs):\n",
    "        preprocess_kwargs = {}\n",
    "        # In context learning if Q&A are given\n",
    "        if \"question\" in kwargs and \"context\" in kwargs:\n",
    "            preprocess_kwargs[\"question\"] = kwargs[\"question\"]\n",
    "            preprocess_kwargs[\"context\"] = kwargs[\"context\"]\n",
    "        return preprocess_kwargs, {}, {}\n",
    "\n",
    "    def preprocess(self, inputs, question=None, context=None):\n",
    "        # Create in context prompt\n",
    "        if question and context:\n",
    "            inputs = in_context_prompt(question=question, context=context)\n",
    "        \n",
    "        # Tokenize input\n",
    "        model_inputs = self.tokenizer(inputs, padding=True, truncation=True, return_tensors='pt')\n",
    "        \n",
    "        return model_inputs\n",
    "\n",
    "    def _forward(self, model_inputs):\n",
    "        output_tokens = self.model(**model_inputs)\n",
    "        return output_tokens\n",
    "\n",
    "    def postprocess(self, output_tokens):\n",
    "        outputs = self.tokenizer.convert_tokens_to_string(output_tokens)\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a3ac94e-8c50-490f-80bf-1026b09fc92c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"HuggingFaceH4/zephyr-7b-alpha\"\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained(model_id)\n",
    "model = transformers.AutoModelForCausalLM.from_pretrained(model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4271c959-f397-4508-bb36-352a3e9f5b9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Probably not possible to be flexible in pipeline"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
